# BountyHound Local - Single Model Configuration (Recommended)
# GPU: 1x H100 NVL (94GB) or H100 SXM/PCIe (80GB)
# Profile: Single 72B AWQ handles all roles via system prompts
# VRAM: ~40GB model + ~40-50GB KV cache
# Benefit: Rock-solid stability, no multi-process memory contention
#
# All 7 roles use the same model on port 8100.
# Different system prompts (src/models/prompts/) specialize each role.
# vLLM batches concurrent requests automatically.

inference:
  engine: vllm
  host: "0.0.0.0"
  base_port: 8100

models:
  orchestrator:
    name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
    role: "orchestrator"
    port: 8100
    gpu_memory_utilization: 0.90
    max_model_len: 32768
    tensor_parallel_size: 1
    quantization: awq
    dtype: auto
    description: "Main brain - target selection, planning, decision making"

  discovery:
    name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
    role: "reasoning"
    port: 8100
    gpu_memory_utilization: 0.90
    max_model_len: 32768
    tensor_parallel_size: 1
    quantization: awq
    dtype: auto
    description: "Hypothesis generation, pattern synthesis, anomaly detection"

  auth:
    name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
    role: "reasoning"
    port: 8100
    gpu_memory_utilization: 0.90
    max_model_len: 32768
    tensor_parallel_size: 1
    quantization: awq
    dtype: auto
    description: "Account creation, token extraction, auth management"

  exploit:
    name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
    role: "code"
    port: 8100
    gpu_memory_utilization: 0.90
    max_model_len: 32768
    tensor_parallel_size: 1
    quantization: awq
    dtype: auto
    description: "Exploit crafting, payload generation, code analysis"

  validator:
    name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
    role: "validation"
    port: 8100
    gpu_memory_utilization: 0.90
    max_model_len: 32768
    tensor_parallel_size: 1
    quantization: awq
    dtype: auto
    description: "PoC validation, curl-based verification, binary verdicts"

  reporter:
    name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
    role: "reporting"
    port: 8100
    gpu_memory_utilization: 0.90
    max_model_len: 32768
    tensor_parallel_size: 1
    quantization: awq
    dtype: auto
    description: "Report generation, severity calibration, platform formatting"

  fast:
    name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
    role: "utility"
    port: 8100
    gpu_memory_utilization: 0.90
    max_model_len: 32768
    tensor_parallel_size: 1
    quantization: awq
    dtype: auto
    description: "Quick parsing, output formatting, simple classification"
